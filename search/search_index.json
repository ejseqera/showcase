{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Seqera Platform: Demonstration Walkthrough","text":""},{"location":"#overview","title":"Overview","text":"<p>This guide provides a walkthrough of a standard Seqera Platform demonstration. The demonstration will describe how to add and run a pipeline in the Platform, examine the run details, as well as highlight key features such as pipeline optimization, Data Explorer and Data Studios.</p> <p>The demonstration will focus on using the nf-core/rnaseq pipeline as an example to execute a Nextflow pipeline on Seqera Cloud via the AWS Batch cloud executor.</p>"},{"location":"#requirements","title":"Requirements","text":"<p> Login to Seqera Platform  Seqera Main Site</p> <p> Seqera Cloud account</p> <p> Access to a Workspace in Seqera Cloud</p> <p>  Access to an AWS Batch Compute Environment created in that Workspace</p> <p> Publicly available nf-core/rnaseq pipeline repository</p> <p> Input samplesheet to run the nf-core/rnaseq pipeline on Seqera Cloud</p>"},{"location":"001_what_is_nextflow/","title":"What is Nextflow?","text":""},{"location":"001_what_is_nextflow/#introduction-to-nextflow","title":"Introduction to Nextflow","text":"<p>Nextflow is a domain specific language (DSL) for enabling scalable, portable, and reproducible workflows.</p> <p>Nextflow is both a workflow language and an execution runtime that supports a wide range of execution platforms, including popular traditional grid scheduling systems such as Slurm and IBM LSF, and cloud services such as AWS Batch, Google Cloud Batch, Azure Batch and Kubernetes.</p> <p>While Nextflow solves many of the technical challenges associated with building and executing data pipelines, bioinformaticians, data scientists, and clinicians still face challenges.</p> <ul> <li>Users shouldn't need extensive technical knowledge of the command line, and understanding of how to configure computing environments to monitor pipeline execution.</li> <li>Researchers also face difficulties in ensuring reproducibility, tracking data provenance, and sharing comprehensive reports and interactive tools for data analysis.</li> </ul>"},{"location":"001_what_is_nextflow/#limitations","title":"Limitations","text":"<p>Monitoring and launching workflows via the Nextflow CLI, though direct, poses challenges, especially with complex or large-scale pipelines that are not as simple as running a Hello World pipeline:</p> <ul> <li> <p>Scalability: As the number of tasks increases, manually checking individual log files becomes impractical.</p> </li> <li> <p>Real-Time Tracking: The CLI does not offer an easy way to visualize real-time progress across multiple parallel tasks.</p> </li> <li> <p>Aggregation: Collecting and interpreting logs from various processes requires additional tools or scripts, complicating the workflow management.</p> </li> <li> <p>Flexibility: Switching between environments (i.e., your local computer to HPC, or cloud) requires the setup of access in the form of account keys and credentials to the environment on your CLI, followed by using the appropriate Nextflow configuration settings.</p> </li> </ul>"},{"location":"002_what_is_the_seqera_platform/","title":"What is the Seqera Platform?","text":""},{"location":"002_what_is_the_seqera_platform/#introduction-to-the-seqera-platform","title":"Introduction to the Seqera Platform","text":"<p>The Seqera Platform is an intuitive centralized command post designed to make scientific analysis accessible at any scale. </p> <p>The Platform acts as a pane of glass, enabling users to effortlessly launch, manage, monitor, and collaborate on scalable Nextflow data analysis using their own computing resources and infrastructure. Researchers can focus on the science that matters rather than worrying about infrastructure engineering.</p> <p>The Seqera Platform helps organizations:</p> <ul> <li>Launch, manage, and monitor portable Nextflow pipelines from anywhere in real-time</li> <li>Enable non-technical users to run pipelines via the intuitive Launchpad interface</li> <li>Easily provision and leverage cloud-based compute environments</li> <li>Collaborate and share pipelines and data securely among local and remote teams</li> <li>Easily access libraries of production-proven Nextflow community pipelines available from nf-core</li> <li>Automate complex tasks as part of broader enterprise processes</li> </ul>"},{"location":"002_what_is_the_seqera_platform/#deployment-methods","title":"Deployment Methods","text":"<p>Seqera offers two deployment methods:</p> <ul> <li>Seqera Cloud: A version of the application available as a SaaS solution, hosted on Seqera's infrastructure.</li> <li>Seqera Enterprise: A lightweight, deployable version of the application that can be hosted on our customers infrastructure.</li> </ul>"},{"location":"002_what_is_the_seqera_platform/#core-components","title":"Core components","text":"<p>The Platform consists of three main architectural components: a backend container, a frontend container, and a database that stores all of the data required by the application. The frontend container communicates with the backend container and database via API calls. As a result, all features and activities available through the user interface can also be accessed programmatically via the Seqera Platform API. For more information, please refer to the Automation section later in the walkthrough.</p> <p>This walkthrough will demonstrate the various features of the Seqera Platform which makes it easier to build, launch, and manage scalable data pipelines.</p>"},{"location":"003_accessing_the_platform/","title":"Accessing the Platform","text":""},{"location":"003_accessing_the_platform/#login","title":"Login","text":"<p>Login to Seqera Cloud, either through a GitHub or Google account, or by providing an email address. If you are signing in for the first time, Seqera Cloud will send an authentication link to the email address, enabling you to login.</p> Click to show animation <p></p>"},{"location":"003_accessing_the_platform/#organizations-and-workspaces","title":"Organizations and Workspaces","text":"<p>All resources in the Seqera Platform are managed within Organizations, which will typically be named the same as your organization e.g. Seqera or Johnson and Johnson. An Organization can be formed of multiple Workspaces. Each Workspace is an isolated environment which can consist of different users, pipelines, credentials and compute infrastructure, enabling access to the Platform depending on the organizational needs. Typically, teams of colleagues or collaborators will have access to one or more Workspace, and all resources in a Workspace (i.e. pipelines, compute environments, datasets) are shared by members of that Workspace.</p> <p>With Workspaces, you can:</p> <ul> <li>Help research teams segment their work depending on their needs.</li> <li>Teams can focus on specific activities, such as core R&amp;D or clinical trials with PII, which need to be kept secure.</li> <li>Organization owners can create Workspaces for specific internal departments like oncology, neuroscience, or therapeutics.</li> <li>Users within an Organization can be added to Workspaces as teams or groups.</li> </ul> <p>Navigate to the <code>seqeralabs/showcase</code> Workspace which contains all of the relevant entities required for this walkthrough guide.</p> Click to show animation <p></p>"},{"location":"004_launching_pipelines/","title":"Launching pipelines","text":""},{"location":"004_launching_pipelines/#launchpad","title":"Launchpad","text":"<p>Each Workspace has a Launchpad that allows users to easily create and share Nextflow pipelines that can be executed on any infrastructure supported by the Platform e.g. all public clouds and most HPC schedulers. A Launchpad pipeline consists of a pre-configured workflow repository, compute environment, and launch parameters.</p> <p>Users can create their own pipelines, share them with others on the Launchpad, or tap into over a hundred community pipelines available on nf-core and other sources.</p> Advanced <p>Adding a new pipeline is relatively simple and can be included as part of the demonstration. See the Adding a Pipeline section.</p>"},{"location":"004_launching_pipelines/#launch-the-nf-corernaseq-pipeline","title":"Launch the nf-core/rnaseq pipeline","text":""},{"location":"004_launching_pipelines/#1-go-to-launchpad","title":"1. Go to Launchpad","text":"<p>Navigate to the Launchpad in the <code>seqeralabs/showcase</code> Workspace and select Launch next to the <code>nf-core-rnaseq</code> pipeline to open the launch form.</p> Click to show animation <p></p>"},{"location":"004_launching_pipelines/#2-nextflow-parameter-schema","title":"2. Nextflow parameter schema","text":"<p>On clicking the launch button, a parameters page will become visible to allow you to fine-tune the pipeline execution. This parameters form is rendered from a file called <code>nextflow_schema.json</code> which can be found in the root of the pipeline Git repository. By adding a simple JSON-based schema describing pipeline parameters, the <code>nextflow_schema.json</code> allows pipeline developers to easily adapt their in-house Nextflow pipelines to be executed via the interactive web interface available on the Seqera Platform.</p> <p>Please refer to the \"Best Practices for Deploying Pipelines with the Seqera Platform\" blog for further information on how to automatically build the parameter schema for any Nextflow pipeline using tooling maintained by the nf-core community. </p>"},{"location":"004_launching_pipelines/#3-parameter-selection","title":"3. Parameter selection","text":"<p>The following Platform-specific options can be adjusted if required:</p> <ul> <li> <p><code>Workflow run name</code>:</p> <p>A unique identifier for the run, pre-filled with a random name. This can be customized.</p> </li> <li> <p><code>Labels</code>:</p> <p>Assign new or existing labels to the run. For example, Project ID or genome version.</p> </li> </ul> <p>Each pipeline including nf-core/rnaseq will have it's own set of parameters that need to be provided in order to run it. The following parameters are mandatory:</p> <ul> <li> <p><code>input</code>:</p> <p>Most nf-core pipelines have standardized the usage of the <code>input</code> parameter to specify an input samplesheet that contains paths to any input files (e.g. FastQ files) as well as any additional metadata required to run the pipeline. The <code>input</code> parameter can take a path to a samplesheet in the S3 bucket selected through Data Explorer. Alternatively, the Seqera Platform has a Datasets feature that allows you to upload structured data like samplesheets for use with Nextflow pipelines.</p> <p>For the purposes of this demonstration, click on the \"Browse\" button next to the <code>input</code> parameter, and search and select a pre-loaded Dataset called \"rnaseq_samples\".</p> Click to show animation <p></p> Advanced <p>Users can upload their own samplesheets and make them available as a Dataset by uploading them in the 'Datasets' tab. For more information please refer to the Add a Dataset section.</p> </li> <li> <p><code>outdir</code>:</p> <p>Most nf-core pipelines have standardized the usage of the <code>outdir</code> parameter to specify where the final results created by the pipeline are published. <code>outdir</code> must be different for each different pipeline run, otherwise, your results will be overwritten. Since we want to publish these files to S3, we must provide the path to the appropriate storage location.</p> <p>For the <code>outdir</code> parameter, specify an S3 directory path manually, or select Browse to specify a cloud storage directory using Data Explorer.</p> Click to show animation <p></p> </li> </ul> <p>Users can easily modify and specify other parameters to customize the pipeline execution through the parameters form. For example, in the 'Read trimming options' section of the parameters page, change the <code>trimmer</code> to select <code>fastp</code> in the dropdown menu, instead of <code>trimgalore</code>, and click the \"Launch\" button!</p> <p></p>"},{"location":"005_adding_a_pipeline/","title":"Adding a pipeline to the Launchpad","text":"<p>The Launchpad allows you to create a preconfigured set of Nextflow pipelines that are ready to be executed on any Compute Environment that has been added to a given Workspace. This allows users to launch pipelines and to customize the appropriate pipeline-level parameters without needing to worry about the complexities of the underlying compute infrastructure.</p>"},{"location":"005_adding_a_pipeline/#adding-the-nf-corernaseq-pipeline","title":"Adding the nf-core/rnaseq pipeline","text":"<p>For this walkthrough, we will add the nf-core/rnaseq pipeline to the Launchpad.</p> <p>Click on the <code>Add Pipeline</code> button and specify:</p> <ul> <li>Name: <code>nf-core-rnaseq-yeast</code></li> <li>Description: <code>nf-core/rnaseq pipeline configured for yeast data</code><ul> <li>[Optional] Free text summary of the pipeline that may be useful to users when selecting a pipeline to launch. </li> </ul> </li> <li>Labels: <code>yeast</code><ul> <li>[Optional] Labels allow you to categorize the pipeline according to arbitrary criteria (e.g. reference genome version) that may help users to select the appropriate pipeline for their analysis within the Launchpad.</li> </ul> </li> <li>Compute Environment: <code>seqera_aws_ireland_fusionv2_nvme</code><ul> <li>Select an existing Compute Environment that has already been added to the Workspace. In this case, it is an AWS Batch Compute Environment in Ireland that has been pre-configured to use Fusion version 2.</li> </ul> </li> <li>Pipeline to launch: <code>https://github.com/nf-core/rnaseq</code><ul> <li>The Platform allows you to select any public or private git repository containing Nextflow source code.</li> </ul> </li> <li>Revision number: <code>3.14.0</code><ul> <li>When you provide the Pipeline to launch, the Platform will search all of the available tags and branches withing the upstream pipeline repository and render a dropdown to select the appropriate version. Selecting a specific version is very important for reproducibility, and should ensure generating the same results.</li> </ul> </li> <li>Config profiles: <code>test</code><ul> <li>[Optional] The Platform allows you to select a profile that has been defined within the Nextflow pipeline. All nf-core pipelines have a <code>test</code> profile that is associated with a minimal test dataset that is used to run the pipeline on heavily sub-sampled input data for the purposes of CI/CD and to quickly confirm that the pipeline runs on any given infrastructure. The <code>test</code> profile of the nf-core/rnaseq pipeline was created using yeast data which is why we have added that particular annotatio in the Labels section.</li> </ul> </li> <li>Pipeline parameters:<ul> <li>[Optional] You can set any custom pipeline parameters in this section that will be prepopulated when users launch the pipeline from the Launchpad. For example, you can set the path to local reference genomes so users don't have to worry about locating these files when launching the pipeline.</li> </ul> </li> <li>Pre-run script:<ul> <li>[Optional] You can define Bash code that executes before the pipeline launches in the same environment where Nextflow runs. Pre-run scripts are useful for defining executor settings, troubleshooting as well as defining a specific version of Nextflow via the <code>NXF_VER</code> environment variable.</li> </ul> </li> </ul> <p>Once you have populated the appropriate settings, click the <code>Add</code> button and this pipeline will become available for other users in the same Workspace to launch within the preconfigured compute infrastructure.</p> <p></p> <p></p> <p></p>"},{"location":"006_adding_a_dataset/","title":"Datasets","text":"<p>Most bioinformatics pipelines will require an input of some sort, typically a samplesheet where each row consists of a sample, the location of files for that sample (such as fastq files), and other sample details.</p> <p>On the Cloud, users might have to upload this samplesheet to a bucket to be used as input to a pipeline, or they will have to retrieve the path to this file on a shared filesystem.</p> <p>Instead, these samplesheets can be made easily accessible through the 'Datasets' feature on the Platform. Datasets in Seqera Platform allow users to upload a structued CSV or TSV file to a Workspace. They are then used as inputs to pipelines to simplify data management, minimize user data-input errors, and facilitate reproducible workflows.</p>"},{"location":"006_adding_a_dataset/#1-download-the-nf-corernaseq-test-samplesheet","title":"1. Download the nf-core/rnaseq test samplesheet","text":"<p>The nf-core/rnaseq pipeline works with input datasets (samplesheets) containing sample names, fastq file locations, and indications of strandedness. The Seqera Community Showcase sample dataset for nf-core/rnaseq looks like this:</p> <p>Example rnaseq dataset</p> <p> sample fastq_1 fastq_2 strandedness WT_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse WT_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse WT_REP2 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP1 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP2 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP2 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_IAA_30M_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse <p></p> <p>Download the nf-core/rnaseq samplesheet_test.csv provided in this repository on to your computer.</p>"},{"location":"006_adding_a_dataset/#2-add-the-dataset","title":"2. Add the Dataset","text":"<p>Go to the 'Datasets' tab and click 'Add Dataset'.</p> <p></p> <p>Specify a name for the dataset such as 'nf-core-rnaseq-test-dataset', description, include the first row as header, and upload the CSV file provided in this repository. This CSV file specifies the paths to 7 small FASTQ files for a sub-sampled Yeast RNAseq dataset.</p> <p>Notice the location of the files point to a path on S3. This could also be a path to a shared filesystem if using HPC. By providing the path to the files, Nextflow will stage the files into the task working directory. </p> Note <p>Seqera Platform will not store any data used for analysis in pipelines. The Datasets should include locations of data that is stored elsewhere, on the user's infrastructure.</p>"},{"location":"007_monitoring_runs/","title":"Monitoring runs","text":"<p>There are several ways to monitor run executions within the Seqera Platform.</p>"},{"location":"007_monitoring_runs/#1-workspace-view","title":"1. Workspace view","text":"<p>A full history of all pipelines executions within a given Workspace can be accessed via the Runs tab:</p> Click to show animation <p></p>"},{"location":"007_monitoring_runs/#2-all-runs-view","title":"2. All runs view","text":"<p>The All runs page can be accessed from the user menu on the top right of the interface. This page provides a comprehensive overview of the runs across the entire Platform instance. The default view will be all Organizations and Workspaces accessible by a given user. However, you can select the visible Workspaces by clicking on the dropdown next to View, and filter for a particular set of runs using any of the following fields:</p> <ul> <li><code>status</code></li> <li><code>label</code></li> <li><code>workflowId</code></li> <li><code>runName</code></li> <li><code>username</code></li> <li><code>projectName</code></li> <li><code>after: YYYY-MM-DD</code></li> <li><code>before: YYYY-MM-DD</code></li> <li><code>sessionId</code></li> <li><code>is:starred</code></li> </ul> <p>For example:</p> <pre><code>rnaseq username:johndoe status:succeeded after:2024-01-01\n</code></pre> Click to show animation <p></p>"},{"location":"007_monitoring_runs/#3-dashboard-view","title":"3. Dashboard view","text":"<p>The Dashboard page can be accessed from the user menu on the top right of the interface. This page provides an overview of the total runs that are currently submitted, running, or have failed. This page provides a comprehensive overview of the runs across the entire Platform instance. The default view will be all Organizations and Workspaces accessible by a given user. However, you can select the visible Workspaces by clicking on the dropdown next to View, and filter by time, including a custom date range up to 12 months. You can also export this data in CSV format by clicking the Export data button.</p> Click to show animation <p></p>"},{"location":"008_viewing_run_information/","title":"Run Information","text":"<p>Upon launching a Pipeline, you'll be navigated to the 'Runs' tab which contains all executed workflows. </p> <p>The Runs tab contains all previous job executions. Each new or resumed job is given a random name, e.g., \"grave_williams\". Each row corresponds to a specific job. As a job executes, it can transition through the following states:</p> <ul> <li>submitted: Pending execution</li> <li>running: Running</li> <li>succeeded: Completed successfully</li> <li>failed: Successfully executed, where at least one task failed with a terminate error strategy</li> <li>cancelled: Stopped forceably during execution</li> <li>unknown: Indeterminate status</li> </ul> Click to show animation <p></p>"},{"location":"008_viewing_run_information/#1-view-the-run-details-for-nf-corernaseq","title":"1. View the run details for nf-core/rnaseq","text":"<p>The pipeline launched in the previous step will take some time to begin running. In the meantime, we can take a look at a previous successful run to observe the run details.</p> <p>Click on a previous run to go to the Run Details.</p>"},{"location":"008_viewing_run_information/#2-run-details-page","title":"2. Run details page","text":"<p>As the pipeline begins to run, you will see the Runs page become populated with the following tabs:</p> <ul> <li> <p>Command-line: The Nextflow command invocation that would be used to run the pipeline. This contains details about the version (through the <code>-r</code> flag), and profile, if specified (through the <code>-profile</code> flag).</p> </li> <li> <p>Parameters: Exact set of parameters used in the execution. This is helpful for reproducing results of a previous run.</p> </li> <li> <p>Resolved Nextflow configuration: The full Nextflow configuration settings used for the run. This includes parameters, but also settings specific to task execution (i.e. memory, CPUs, output directory).</p> </li> <li> <p>Execution Log: A summarized Nextflow log providing information about the pipeline, and the status of the run.</p> </li> <li> <p>Datasets: Link to Datasets if any were used in the run.</p> </li> <li> <p>Reports: View outputs of your pipeline directly in the Platform.</p> </li> </ul> Click to show animation <p></p>"},{"location":"008_viewing_run_information/#3-view-reports","title":"3. View Reports","text":"<p>Most Nextflow pipelines will generate reports or output files which are useful to inspect at the end of the pipeline execution. </p> <p>Reports can contain quality control (QC) metrics that would be important to assess the integrity of the results.</p> <p>For example, for the nf-core/rnaseq pipeline, you can view the MultiQC report generated. MultiQC is helpful reporting tool to generate aggregate statistics and summaries from bioinformatics tools.</p> <p></p> <p></p> <p>Notice the path to the file will still point to a location on the user's infrastructure, but we are able to more transparently see the contents and download the file, without having to go to the Cloud or a remote filesystem.</p>"},{"location":"008_viewing_run_information/#specifying-outputs-in-reports","title":"Specifying outputs in Reports","text":"<p>To customize and instruct the Platform on where to find reports generated by the pipeline, the Platform requires a YAML file tower.yml that contains the locations of the generated reports. </p> <p>In the nf-core/rnaseq pipeline, the MULTIQC process step generates a MultiQC report file in HTML format.</p> <pre><code>reports:\n  multiqc_report.html:\n    display: \"MultiQC HTML report\"\n</code></pre>"},{"location":"008_viewing_run_information/#4-view-general-information","title":"4. View general information","text":"<p>On the Runs page will be General information about who executed the run, when, the Git hash used and tag, as well as additional details about the compute environment used, and the version of Nextflow.</p> Click to show animation <p></p> <p>The 'General' panel displays top-level information about a pipeline run:</p> <ul> <li>Unique workflow run ID</li> <li>Workflow run name</li> <li>Timestamp of pipeline start</li> <li>Project revision and Git commit ID</li> <li>Nextflow session ID</li> <li>Username of the launcher</li> <li>Work directory path</li> </ul>"},{"location":"008_viewing_run_information/#5-view-details-for-a-task","title":"5. View details for a Task","text":"<p>Scroll down the Runs page and you will see:</p> <ul> <li>The progress of each Process in the pipeline</li> <li>Aggregated stats for the Run (i.e. total walltime, CPU hours)</li> <li>Workflow metrics (i.e. CPU efficiency, memory efficiency)</li> <li>Task details table for every task in the workflow</li> </ul> <p>The task details table can provide further information of every step in the pipeline, what the status of the task is, and metrics for the task.</p>"},{"location":"008_viewing_run_information/#6-task-details-window","title":"6. Task details window","text":"<p>Select a task in the task table to open the Task details dialog. The dialog has three tabs: About, Execution log and Data Explorer.</p> <p>About</p> <p>The About tab provides the following information:</p> <ol> <li> <p>Name: Process name and tag</p> </li> <li> <p>Command: Task script, defined in the pipeline process</p> </li> <li> <p>Status: Exit code, task status, attempts</p> </li> <li> <p>Work directory: Directory where the task was executed</p> </li> <li> <p>Environment: Environment variables that were supplied to the task</p> </li> <li> <p>Execution time: Metrics for task submission, start, and completion time</p> </li> <li> <p>Resources requested: Metrics for the resources requested by the task</p> </li> <li> <p>Resources used: Metrics for the resources used by the task</p> </li> </ol> Click to show animation <p></p> <p>Execution log</p> <p>The Execution log tab provides a real-time log of the selected task's execution. Task execution and other logs (such as stdout and stderr) are available for download from here, if still available in your compute environment.</p>"},{"location":"008_viewing_run_information/#7-task-work-directory-in-data-explorer","title":"7. Task work directory in Data Explorer","text":"<p>If a task fails, a good place to begin troubleshooting is taking a look at the task's work directory.</p> <p>Nextflow hash-addresses each task of the pipeline and creates unique directories based on these hashes. Instead of navigating through a bucket on the Cloud console or filesystem to find the contents of this directory, we can make use of the 'Data Explorer' tab in the Task window.</p> <p>The Data Explorer tab allows you to view the log files and output files generated for each task in it's working directory, directly within the Platform.</p> <p>You can view, download, and retrieve the link for these intermediate files stored in the Cloud from the Explorer tab - making troubleshooting much simpler.</p> Click to show animation <p></p>"},{"location":"008_viewing_run_information/#8-resume-a-pipeline","title":"8. Resume a Pipeline","text":"<p>Seqera Platform enables you to use Nextflow's resume functionality to resume a workflow run with the same parameters, using the cached results of previously completed tasks and only executing failed and pending tasks.</p> Click to show animation <p></p> <p>To resume a failed or cancelled run:</p> <ul> <li>Click on the three dots next to the Run</li> <li>Select 'Resume' from the options menu</li> <li>Edit the parameters before launch, if desired</li> <li>If you have the appropriate permissions, you may edit the compute environment if desired</li> </ul>"},{"location":"009_optimizing_pipelines/","title":"Optimize a Pipeline","text":""},{"location":"009_optimizing_pipelines/#resource-usage-and-pipeline-optimization","title":"Resource Usage and Pipeline Optimization","text":"<p>The task-level resource usage metrics provided by the Platform allow you to determine the resources requested for a task and what was actually used. This information helps you fine-tune your configuration more accurately.</p> <p>However, manually adjusting resources for every task in your pipeline is impractical. Instead, you can leverage the pipeline optimization feature available on the Launchpad.</p> <p>Seqera's pipeline optimization feature analyzes resource usage data from previous runs to optimize resource allocation for future runs. After a successful run, this optimization feature becomes available, indicated by the grey lightbulb icon turning into a black-hashed lightbulb.</p>"},{"location":"009_optimizing_pipelines/#1-optimize-nf-corernaseq","title":"1. Optimize nf-core/rnaseq","text":"<p>Navigate back to the Launchpad, click on the nf-core/rnaseq Pipeline, and click on the 'Lightbulb' icon to view the optimized profile. You have the flexibility to tailor the optimization's target settings and incorporate a retry strategy as needed.</p>"},{"location":"009_optimizing_pipelines/#2-view-optimized-configuration","title":"2. View optimized configuration","text":"<p>When clicking on the lightbulb, you will get access to an optimized configuration profile in the second tab.</p> <p>This profile consists of Nextflow configuration settings for each process and each resource directive (where applicable):  cpus, memory, and time. The optimized setting for a given process and resource directive is based on the maximum use of that resource across all tasks in that process.</p> <p>Once optimization is selected, any subsequent runs of that pipeline on the Launchpad will inherit the optimized configuration profile, indicated by the black lightbulb icon with a checkmark. </p> <p>NOTE: Optimizated profiles are generated off of one run at a time, defaulting to the most recent runs, and not an aggregation of previous runs.</p> Click to show animation <p></p> <p>You can verify the optimized configuration of a given run by inspecting the resource usage plots for that run and these fields in the run's task table:</p> Description Key CPU usage <code>pcpu</code> Memory usage <code>peakRss</code> Runtime <code>start</code> and <code>complete</code>"},{"location":"010_using_data_explorer/","title":"Data Explorer","text":"<p>When running pipelines in the Cloud, users typically need access to buckets and blob storage to view pipeline results and upload files (e.g., samplesheets, reference data) for analysis. Managing credentials and permissions for multiple users, as well as training users to navigate Cloud consoles and CLIs, can be complicated. Instead, users can view their data directly through Data Explorer.</p> <p>With Data Explorer, you can browse and interact with remote data repositories from organization workspaces in the Seqera Platform. It supports AWS S3, Azure Blob Storage, and Google Cloud Storage repositories.</p>"},{"location":"010_using_data_explorer/#view-pipeline-outputs-in-data-explorer","title":"View pipeline outputs in Data Explorer","text":"<p>In Data Explorer, you are able to:</p> <ul> <li> <p>View bucket details:     The cloud provider, bucket address, and credentials, by selecting the information icon next to a bucket in the Data Explorer list.</p> Click to show animation <p></p> </li> <li> <p>View bucket contents     Select a bucket name from the Data Explorer list to view the contents of that bucket. </p> <p>The file type, size, and path of objects are displayed in columns to the right of the object name. For example, we can take a look at the outputs of our nf-core/rnaseq run.</p> Click to show animation <p></p> </li> <li> <p>Preview files:      Select a file to open a preview window that includes a Download button. For example, we can use Data Explorer to view the results of the nf-core/rnaseq pipeline that we executed. Specifically, we can take a look at the resultant gene counts of the salmon quantification step:</p> Click to show animation <p></p> </li> </ul>"},{"location":"010_using_data_explorer/#configure-a-bucket-to-browser-in-data-explorer","title":"Configure a bucket to browser in Data Explorer","text":"<p>Data Explorer also enables you to add public cloud storage buckets to view and use data from resources such as:</p> <ul> <li>The Cancer Genome Atlas (TCGA)</li> <li>1000 Genomes Project</li> <li>NCBI SRA</li> <li>Genome in a Bottle Consortium</li> <li>MSSNG Database</li> <li>Genome Aggregation Database (gnomAD) </li> </ul>"},{"location":"010_using_data_explorer/#1-add-a-cloud-bucket","title":"1. Add a cloud bucket","text":"<p>Select 'Add cloud bucket' from the Data Explorer tab to add individual buckets (or directory paths within buckets). </p>"},{"location":"010_using_data_explorer/#2-fill-in-bucket-details","title":"2. Fill in bucket details","text":"<p>Specify the Provider, Bucket path, Name, Credentials, and Description, then select Add. For public cloud buckets, select Public from the Credentials drop-down menu.</p> Click to show animation <p></p> <p>You are now able to use this data in your analysis without having to interact with Cloud consoles or CLI tools. </p>"},{"location":"011_tertiary_analysis_data_studios/","title":"Tertiary Analysis in Data Studios","text":""},{"location":"011_tertiary_analysis_data_studios/#introduction-to-data-studios","title":"Introduction to Data Studios","text":"<p>After running a pipeline, you may want to perform tertiary analysis using platforms like Jupyter Notebook or RStudio. Setting up the infrastructure for these platforms, including accessing pipeline data, results, and necessary bioinformatics packages, can be complex and time-consuming.</p> <p>Data Studios streamlines this process for Seqera Platform users by allowing them to add interactive analysis environments based on templates, similar to how they add and share Pipelines and Datasets.</p> <p>The Seqera Platform manages all the details, enabling users to easily select their preferred interactive tool and analyze their data within the platform.</p> <p>On the 'Data Studios' tab, you will be able to monitor and see the details of Studios in a Workspace.</p> <p>Studios will have a name, followed by the cloud provider they are run on, the container image being used (Jupyter, VS Code, RStudio), the user who created the Studio, the timestamp for creation, followed by the status to indicate whether it has Started, Stopped, or is Running. </p> <p></p> <p>Clicking on the three dots will allow you to: - See the details of the Studio - Connect to the Studio - Start the studio - Stop the studio - Copy the Data Studio URL</p>"},{"location":"011_tertiary_analysis_data_studios/#analyse-rnaseq-data-in-data-studios","title":"Analyse RNAseq Data in Data Studios","text":"<p>We can use Data Studios to perform bespoke analysis on the results of upstream workflows. For example we can run the nf-core/rnaseq workflow to quantify gene expression, followed by nf-core/differentialabundance to derive differential expression statistics, and then use Data Studios to interrogate and visualise the results of those analyses. </p> <p>As an example, we have run RNAseq results through the differentialabundance pipeline and created a new Data Studio with these results from the cloud mounted into the Studio to perform further analysis. One of these outputs is a Shiny application, which we can deploy for interactive analysis.</p> Presenter's Note <p>The Data Studios demonstration involves deploying a RShiny app to explore RNAseq results. This will open the app in a new browser window. You may choose to have this window already open before your demo and switch over to it.</p>"},{"location":"011_tertiary_analysis_data_studios/#1-open-the-rnaseq-analysis-studio","title":"1. Open the RNAseq Analysis Studio","text":"<p>Click on the rnaseq_to_differentialabundance Studio.</p> <p>Upon clicking on the Studio, you will see we are able to create an RStudio environment that uses an existing Compute Environment available in this Workspace. </p> <p>We have also mounted data generated from our RNAseq pipeline and subsequent differentialabundance pipeline, directly from AWS S3. </p> <p>We can also specify the resources this Studio will use. </p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#2-connect-to-the-studio","title":"2. Connect to the Studio","text":"<p>This Data Studio will start an RStudio environment in which we have already installed the necessary R packages for deploying an RShiny app that will allow us to interact with various comparisons and visualisations of our RNAseq data. We've also generated an R Markdown document with the commands in place to generate the R Shiny application.</p> Click to show animation <p></p> <p>We can deploy the RShiny app in the Data Studio by clicking on the green play button on the last \"chunk\" or section of the R script:</p> <p></p>"},{"location":"011_tertiary_analysis_data_studios/#3-explore-results-in-rshiny-app","title":"3. Explore results in RShiny app","text":"<p>In a separate browser window, the RShiny app will deploy to provide an interface where you can interact with the data.</p> <p>You will be able to see information about your sample data, perform QC/exploratory analysis, and take a look at results of differential expression analyses.</p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#exploring-sample-clustering-with-pca","title":"Exploring Sample Clustering with PCA","text":"<p>Under the 'QC/Exploratory' tab, you can click on the PCA (Principal Component Analysis) plot to visualize how the samples group together based on their gene expression profiles.</p> <p>In this example, we used RNA-seq data from the publicly available ENCODE project, which includes samples from four different cell lines: GM12878 (a lymphoblastoid cell line), K562 (a chronic myelogenous leukemia cell line), MCF-7 (a breast cancer cell line), and H1-hESC (human embryonic stem cells).</p> <p>Here\u2019s what to look for in the PCA plot:</p> <ul> <li> <p>Replicate Clustering: Ideally, replicates of the same cell type should cluster closely together. For example, you should see the replicates of MCF-7 (breast cancer cell line) grouping together. This indicates consistent gene expression profiles among replicates.</p> </li> <li> <p>Cell Type Separation: Different cell types should form distinct clusters. For instance, GM12878, K562, MCF-7, and H1-hESC samples should each form their own separate clusters, reflecting their unique gene expression patterns.</p> </li> </ul> <p>Using this PCA plot, you can gain insights into the consistency and quality of your RNA-seq data, identify any potential issues, and understand the major sources of variation among your samples - directly in the Platform.</p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#exploring-gene-expression-changes-with-volcano-plots","title":"Exploring Gene Expression Changes with Volcano Plots","text":"<p>Under the 'Differential' tab, you can click on 'Volcano plots' to compare genes with significant changes in expression between two samples. For example, you can filter for 'Type: H1 vs MCF-7' to view the differences in expression between these two cell lines.</p> <ol> <li> <p>Identifying Upregulated and Downregulated Genes: The x-axis of the volcano plot represents the log2 fold change in gene expression between the H1 and MCF-7 samples, while the y-axis represents the statistical significance of the changes.</p> <ul> <li>Upregulated Genes in MCF-7: Genes on the left side of the plot (negative fold change) are upregulated in the MCF-7 samples compared to H1. For example, you may notice the SHH gene, which is known to be upregulated in cancer cell lines, prominently appearing here.</li> </ul> </li> <li> <p>Filtering for Specific Genes: If you are interested in specific genes, you can use the filter function. For example, you can filter for the SHH gene in the table below the plot. This will allow you to quickly locate and examine this gene in more detail.</p> </li> <li> <p>Gene Expression Bar Plot: After filtering for the SHH gene, click on it to navigate to a gene expression bar plot. This plot will show you the expression levels of SHH across all samples, allowing you to see in which samples it is most highly expressed.</p> <ul> <li>In this case, you will see that SHH is most highly expressed in MCF-7, which aligns with its known role in cancer cell proliferation.</li> </ul> </li> </ol> <p>Using the volcano plot, you can effectively identify and explore the genes with the most significant and meaningful changes in expression between your samples, providing a deeper understanding of the molecular differences.</p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#4-collaborate-in-the-studio","title":"4. Collaborate in the Studio","text":"<p>If you wanted to share the results of your RNAseq analysis or allow colleagues to perform exploratory analysis as we just did, you can share a link to the Data Studio by selecting the three dots next to the status message for the data studio you want to share, then select Copy data studio URL. Using this link other authenticated users with the \"Connect\" role at minimum, can access the session directly.</p>"},{"location":"011_tertiary_analysis_data_studios/#5-takeaway","title":"5. Takeaway","text":"<p>This example demonstrates how Data Studios allows you to perform interactive analysis and explore the results of your secondary data analysis all within one unified platform. It simplifies the setup and data management process, making it easier for you to gain insights from your data efficiently.</p>"},{"location":"012_setting_up_data_studio/","title":"Setting up a Data Studio","text":""},{"location":"012_setting_up_data_studio/#data-studio-setup","title":"Data Studio Setup","text":""},{"location":"012_setting_up_data_studio/#create-a-data-studio","title":"Create a Data Studio","text":""},{"location":"012_setting_up_data_studio/#hidden-heading","title":"1. Add a Data Studio","text":"<p>To create a Data Studio, click on the 'Add data studio' button and select from any one of the three currently available templates.</p> Click to show animation <p></p>"},{"location":"012_setting_up_data_studio/#hidden-heading","title":"2. Select a compute environment","text":"<p>Currently, only AWS Batch is supported.</p>"},{"location":"012_setting_up_data_studio/#hidden-heading","title":"3. Mount data using Data Explorer","text":""},{"location":"012_setting_up_data_studio/#create-a-data-link","title":"Create a Data Link","text":"<p>To enable access to data in a Studio, we can create a custom data link pointing to the directory in our AWS S3 bucket where the results are saved. This will allow us to read and write on only the data we need from Cloud Storage, within our Studio.</p> <p>This can be achieved by using the 'Add cloud bucket' button in Data Explorer and specifying the path to our output directory:</p> <p></p>"},{"location":"012_setting_up_data_studio/#mount-the-data-link-into-the-studio","title":"Mount the Data Link into the Studio","text":"<p>Select data to mount into your data studios environment using the Fusion file system in Data Explorer. In the Data Explorer, you can select for the newly created Data Link to mount.</p> <p>This data will be available at <code>/workspace/data/&lt;dataset&gt;</code>.</p> Click to show animation <p></p>"},{"location":"012_setting_up_data_studio/#hidden-heading","title":"4. Resources for environment","text":"<p>Enter a CPU or memory allocation for your data studios environment (optional). The default is 2 CPUs and 8192 MB of memory.</p> <p>Then, click Add!</p> <p>The data studio environment will be available in the Data Studios landing page with the status 'stopped'. Click on the three dots and Start to begin running the studio.</p> Click to show animation <p></p> <p></p>"},{"location":"012_setting_up_data_studio/#connect-to-a-data-studio","title":"Connect to a Data Studio","text":"<p>To connect to a running data studio session, select the three dots next to the status message and choose Connect. A new browser tab will open, displaying the status of the data studio session. Select Connect. </p>"},{"location":"012_setting_up_data_studio/#collaborate-in-data-studio","title":"Collaborate in Data Studio","text":"<p>Collaborators can also join a data studios session in your workspace. For example, to share the results of the nf-core/rnaseq pipeline, you can share a link by selecting the three dots next to the status message for the data studio you want to share, then select Copy data studio URL. Using this link other authenticated users with the \"Connect\" role at minimum, can access the session directly.</p> <p></p>"},{"location":"012_setting_up_data_studio/#stop-a-data-studio","title":"Stop a Data Studio","text":"<p>To stop a running session, click on the three dots next to the status and select Stop. Any unsaved analyses or results will be lost.</p> <p></p> Advanced <p>For a more detailed use-case of performing tertiary analysis with the results of the nf-core/rnaseq pipeline in an RStudio/RShiny app environment, take a look at the Tertiary analysis with Data Studios section.</p>"},{"location":"012_setting_up_data_studio/#checkpoints-in-data-studios","title":"Checkpoints in Data studios","text":"<p>When starting a data studio, a checkpoint gets created. This checkpoint allows you to restart a data studio with previously installed software and changes made to the root filesystem of the container. Please note, that if you stop a data studio and restart it, this will restart it from the latest checkpoint. To go back to a specific previous configuration of data studio session, please restart it from a checkpoint as highlighted in the screenshot below:</p> <p></p>"},{"location":"012_setting_up_data_studio/#more-information","title":"More information","text":"<p>If you want more detailed explanation about specific concepts of Data Studios or find out which tools are preinstalled in Data Studio images, please visit Seqera Platform Docs</p> Advanced <p>To see additional details on Data Studios based on a demonstration from Rob Newman, please visit this hidden section.  </p>"},{"location":"013_data_studios_deep_dive/","title":"Data Studios Deep Dive","text":"<p>This content is transcribed from a Data Studios demo presented by Rob Newman. </p>"},{"location":"013_data_studios_deep_dive/#data-storage-and-data-links","title":"Data Storage and Data Links","text":"<ol> <li> <p>Creating Custom Data Links:</p> <ul> <li>Use the Data Explorer to add a specific data directory.</li> <li>Click on 'Add cloud bucket' and specify the exact path to your data.</li> <li>Note: Any data link added to a Studio session is read/write.</li> </ul> </li> <li> <p>Directory Isolation:</p> <ul> <li>Once a directory is mounted to a Studio session, no one else can access it.</li> <li>This isolation prevents others from overwriting your results.</li> <li>Important for ensuring that only designated scientists or bioinformaticians can work with specific project directories.</li> </ul> </li> <li> <p>Fusion Symlinks Limitation:</p> <ul> <li>Fusion symlinks will not work outside the specified directory.</li> </ul> </li> <li> <p>Allowed Buckets:</p> <ul> <li>Ensure the buckets you want to access through Studios are listed in the 'Allowed Buckets' section in the Compute Environment (CE).</li> </ul> </li> </ol>"},{"location":"013_data_studios_deep_dive/#compute-environment-resources","title":"Compute Environment Resources","text":"<ol> <li> <p>Resource Management:</p> <ul> <li>If using a Compute Environment (CE) for both pipelines and Studio sessions, be aware they will compete for resources.</li> <li>To avoid stalling or losing work due to lack of CPU or memory, consider using a separate CE for Studio sessions.</li> </ul> </li> <li> <p>Handling Large Files:</p> <ul> <li>Staging large files (e.g., BAM files) can crash your session if there aren't enough resources.</li> <li>Ensure your Studio has adequate resources before working with large datasets.</li> </ul> </li> <li> <p>Identifying Studio Sessions:</p> <ul> <li>Each Studio session has a name, which can be identified in AWS Batch within the appropriate CE as the running job name.</li> </ul> </li> </ol>"},{"location":"013_data_studios_deep_dive/#data-studios-infrastructure-and-mechanics","title":"Data Studios Infrastructure and Mechanics","text":"<ol> <li> <p>Container Web Server:</p> <ul> <li>Each Data Studio container includes a web server called Tower Connect, which communicates telemetry data to/from the Platform.</li> <li>Custom containers may add a layer that includes this web server.</li> </ul> </li> <li> <p>Snapshots:</p> <ul> <li>Snapshots are created when you first create a session and each time you stop a studio.</li> <li>These snapshots help in saving and restoring your work environment.</li> </ul> </li> </ol>"},{"location":"013_data_studios_deep_dive/#collaboration","title":"Collaboration","text":"<ol> <li> <p>Monitoring Activity:</p> <ul> <li>If a user is inactive for more than 5 minutes, their icon will disappear from the monitoring page, indicating they are not currently collaborating.</li> </ul> </li> <li> <p>Real-Time Collaboration:</p> <ul> <li>Only Jupyter and VS Code support real-time collaboration.</li> <li>RStudio requires a Pro license for real-time collaboration, and discussions with Posit for this capability are ongoing.</li> </ul> </li> </ol>"},{"location":"014_automation_on_the_seqera_platform/","title":"Automation on the Seqera Platform","text":"<p>Seqera Platform provides multiple methods of programmatic interaction allowing you to automate execution of pipelines, chain pipelines together, and integrate the Platform into third-party services of your choosing.</p>"},{"location":"014_automation_on_the_seqera_platform/#1-seqera-platform-api","title":"1. Seqera Platform API","text":"<p>The Seqera Platform public API is the lowest-level method of programmatic interaction. All features available on the user interface can be achieved through the API. </p> <p>The API can be used to trigger the Launch of pipelines based on a file event (i.e. upload of a file to a bucket) or completion of a previous run.</p> <p>The API can be accessed from <code>https://api.cloud.seqera.io</code>.</p> <p>The full list of endpoints is available in Seqera's OpenAPI schema found here. The API requires an authentication token to be specified in every API request. This can be created in your user menu under Your tokens.</p> Click to show animation <p></p> <p>The token is only displayed once. Store your token in a secure place. Use this token to authenticate requests to the API.</p> Advanced <p>For an example of how to use the API to launch a pipeline, we can make the following request using cURL: </p> <pre><code>curl -X POST \"https://api.cloud.seqera.io/workflow/launch?workspaceId=38659136604200\" \\\n    -H \"Accept: application/json\" \\\n    -H \"Authorization: Bearer &lt;your_access_token&gt;\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Accept-Version:1\" \\\n    -d '{\n    \"launch\": {\n        \"computeEnvId\": \"hjE97A8TvD9PklUb0hwEJ\",\n        \"runName\": \"first-time-pipeline-api-byname\",\n        \"pipeline\": \"first-time-pipeline\",\n        \"workDir\": \"s3://nf-ireland\",\n        \"revision\": \"master\"\n    }\n}'\n</code></pre>"},{"location":"014_automation_on_the_seqera_platform/#2-seqera-platform-cli","title":"2. Seqera Platform CLI","text":"<p>For bioinformaticians and scientists more comfortable with the CLI, Seqera Platform also comes with a command line utility called <code>tw</code> to manage resources. </p> <p>The CLI provides an interface to launch pipelines, manage compute environments, retrieve run metadata and monitor runs on the Platform. It provides a Nextflow-like experience for bioinformaticians who prefer the CLI, allows you store Seqera resource configuration (i.e. pipelines, compute environments) as code. The CLI is built on top of the Seqera Platform API but offers is simpler to use than the API. For example, you can refer to resources by name instead of unique identifier.</p> <p></p> <p>The <code>tw</code> CLI installation and usage details can be obtained from this Github repository.</p> Advanced <p>For example, to launch the hello pipeline using the CLI:</p> <pre><code>tw launch hello --workspace seqeralabs/showcase\n</code></pre>"},{"location":"014_automation_on_the_seqera_platform/#3-seqerakit","title":"3. seqerakit","text":"<p><code>seqerakit</code> is a Python wrapper for the Seqera Platform CLI which can be leveraged to automate the creation of all of the entities in Seqera Platform via YAML format configuration file. It can be used to automate creation of entities (i.e. Organization, Workspace to Pipelines and Launching) in one YAML.</p> <p>The key features are:</p> <ul> <li>Simple configuration: All of the command-line options available when using the Seqera Platform CLI can be defined in simple YAML format.</li> <li>Infrastructure as Code: Enable users to manage and provision their infrastructure specifications.</li> <li>Automation: End-to-end creation of entities within Seqera Platform, all the way from adding an Organization to launching pipeline(s) within that Organization.</li> </ul> <p>The <code>seqerakit</code> installation and usage details are available on this Github repository.</p> Advanced <p>For example, to launch the hello pipeline using seqerakit, you can create a YAML file called <code>hello.yaml</code> as follows:</p> <pre><code>launch:\n- name: \"hello-world\"\n    url: \"https://github.com/nextflow-io/hello\"\n    workspace: \"seqeralabs/showcase\"\n</code></pre> <p>Then run seqerakit with:</p> <pre><code>$ seqerakit hello.yaml\n</code></pre>"},{"location":"014_automation_on_the_seqera_platform/#resources","title":"Resources","text":"<p>A common use-case for using the automation methods above is to automatically execute a pipeline as data arrives off a sequencer or integrating Seqera Platform into a broader customer facing application. For a step-by-step guide on setting up this kind of Workflow Automation on Seqera Platform, take a look at this blog post.</p> <p>For examples on how to use the above methods for chaining workflows together, take a look at this blog post on Automating workflows with Seqera Platform.</p>"},{"location":"015_seqera_pipelines/","title":"Seqera Pipelines","text":"<p>Seqera Pipelines is a list of the best, open-source Nextflow workflows. Finding high-quality pipelines is critical, so we\u2019ve created a tightly curated list of the very best workflows to begin with.</p> <p>Every pipeline comes with curated test data, so you can import into Seqera Platform and launch a test run in just a few clicks:</p> Click to show animation <p></p> <p>The pipeline details page provides key information about the workflow. Within one-click, you can add pipelines to your launchpad within Seqera Platform.</p> <p>Simply click the 'Launch' button, specify whether you would like to add the pipeline to a Cloud or Enterprise instance, and provide your user access token for the Platform.</p> <p>You can then select for the Orgaization, Workspace, and Compute Environment for the pipeline. </p> Click to show animation <p></p> <p>Click on the 'Launch Pipeline' tab to see various methods for launching the pipeline.</p> <p></p> <p>If you\u2019re more at home in the terminal, you can use the launch box to grab commands for Nextflow, Seqera Platform CLI, and nf-core/tools.</p>"},{"location":"016_seqera_containers/","title":"Seqera Containers","text":"<p>Containers have revolutionized research by providing portable environments that eliminate compatibility issues across different computing environments.</p> <p>Nextflow supports Docker containers but pipeline developers often face challenges in having to write Dockerfile scripts for each workflow step.</p> <p>Projects like BioContainers offer pre-built images for Bioconda tools but have limitations. Wave, our open-source on-demand container provisioning service, simplifies this process by allowing Nextflow developers to reference conda packages or a bundled Dockerfile, building containers on the fly.</p> <p>Seqera Containers enhance the Wave experience by allowing users to type in the names of desired tools and instantly receive a container URI, usable for any purpose. The image is stored in a cache provided by AWS, ensuring reproducibility and availability for future runs without expiry.</p> <p>Users can:</p> <ol> <li> <p>Request any combination of packages</p> Click to show animation <p></p> </li> <li> <p>Select architecture and image format (i.e. linux/arm64 architecture)</p> Click to show animation <p></p> </li> <li> <p>Users can create Singularity images and download <code>.sif</code> files directly</p> Click to show animation <p></p> </li> </ol> <p>You can view containers you previously build using the 'My Recent Containers' button. </p> <p>Clicking 'View build details' for the container shows the full information of the Dockerfile, conda environment file, and build settings, as well as the complete build logs. Every container includes results from a security scan.</p> Click to show animation <p></p>"},{"location":"017_walkthrough_summary/","title":"One platform for the scientific data analysis life cycle","text":"<p>Throughout this guide, you have experienced how the Seqera Platform streamlines the management, execution, monitoring, and analysis of Nextflow pipelines in the cloud. This centralized and intuitive interface offers numerous advantages:</p> <p> Ease of Access: Enables all users to execute Nextflow pipelines with ease.</p> <p> Simplified Cloud Deployment: Allows for the deployment of pipelines on the cloud without needing to understand the underlying infrastructure.</p> <p> Real-Time Monitoring: Provides the ability to view the progress and outcomes of pipeline runs directly, bypassing the need for direct access to the execution environment.</p> <p> Enhanced Provenance Tracking: Facilitates the logging and tracking of pipeline provenance, enhancing reproducibility.</p> <p> Cloud Data Interaction: Supports seamless interaction with cloud-stored data, eliminating the need for direct cloud console or CLI interactions.</p> <p> Automated Resource Management: Reduces manual resource tuning, preventing allocation errors and optimizing task execution.</p> <p> Collaborative Efficiency: Boosts productivity by enabling researchers to share, collaborate, and interpret results effortlessly, without additional infrastructure overhead.</p> <p>Seqera Platform empowers scientists to conduct high-throughput computing on a large scale, utilizing modern software engineering practices, all from a single, unified location. This guide has outlined how leveraging these capabilities can transform your research productivity and computational efficiency.</p>"},{"location":"018_resources/","title":"Resources","text":""},{"location":"018_resources/#hidden-heading","title":"Quick links","text":"<p> Seqera website</p> <p> Seqera Platform documentation</p> <p> Seqera Platform API</p> <p> Seqera Platform CLI</p> <p> seqerakit</p> <p> Nextflow documentation</p> <p> nf-core website</p>"},{"location":"018_resources/#hidden-heading","title":"Blog posts","text":"<p> Best Practices for Deploying Pipelines with the Seqera Platform</p> <p> Breakthrough performance and cost-efficiency with the new Fusion file system</p> <p> Workflow Automation for Nextflow Pipelines</p>"}]}