{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Seqera Platform: Demonstration Walkthrough","text":"Login to Seqera Platform             Visit Seqera Main Site"},{"location":"#overview","title":"Overview","text":"<p>This guide provides a walkthrough of a standard Seqera Platform demonstration. The demonstration will describe how to add a pipeline to the Launchpad, launch a workflow with pipeline parameters, monitor a Run, and examine the run details in several different parts. The demonstration will also highlight key features such as the Pipeline Optimization, Data Explorer, and Compute Environment creation.</p> <p>More specifically, this demonstration will focus on using the nf-core/rnaseq pipeline as an example and executing the workflow on AWS Batch.</p>"},{"location":"#requirements","title":"Requirements","text":"<p> A Seqera Platform Cloud account</p> <p> Access to a Workspace in Seqera Platform</p> <p>  An AWS Batch Compute Environment created in that Workspace</p> <p> The nf-core/rnaseq pipeline repository</p> <p> Samplesheet to create a Dataset on the Platform used to run minimal test RNAseq data (see samplesheet_test.csv file in this repository)</p>"},{"location":"#sections","title":"Sections","text":"<p> Why use Seqera Platform? Overview of the Platform Add a Pipeline to the Launchpad Add a Dataset to Seqera Platform Launch a Pipeline Monitoring your run Monitoring views Examine run and task details Resume a Pipeline Data Explorer Data Studios Optimize your Pipeline Automation on Seqera Platform Scaling Science on Seqera Platform </p>"},{"location":"add_a_dataset/","title":"Datasets","text":"<p>Most bioinformatics pipelines will require an input of some sort, typically a samplesheet where each row consists of a sample, the location of files for that sample (such as fastq files), and other sample details.</p> <p>Datasets in Seqera Platform are CSV (comma-separated values) and TSV (tab-separated values) files stored in a workspace. They are used as inputs to pipelines to simplify data management, minimize user data-input errors, and facilitate reproducible workflows.</p> <p>When running pipelines on the Cloud, this samplesheet has to be made available in Cloud storage or a remote location. Instead of doing this, we can upload a samplesheet we have locally, as a Dataset to the Platform to specify as input to our pipeline.</p>"},{"location":"add_a_dataset/#1-download-the-nf-corernaseq-test-samplesheet","title":"1. Download the nf-core/rnaseq test samplesheet","text":"<p>The nf-core/rnaseq pipeline works with input datasets (samplesheets) containing sample names, fastq file locations, and indications of strandedness. The Seqera Community Showcase sample dataset for nf-core/rnaseq looks like this:</p> <p>Example rnaseq dataset</p> <p> sample fastq_1 fastq_2 strandedness WT_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse WT_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse WT_REP2 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP1 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP2 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP2 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_IAA_30M_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse <p></p> <p>Download the nf-core/rnaseq samplesheet_test.csv provided in this repository on to your computer.</p>"},{"location":"add_a_dataset/#2-add-the-dataset","title":"2. Add the Dataset","text":"<p>Go to the 'Datasets' tab and click 'Add Dataset'.</p> <p></p> <p>Specify a name for the dataset such as 'nf-core-rnaseq-test-dataset', description, include the first row as header, and upload the CSV file provided in this repository. This CSV file specifies the paths to 7 small FASTQ files for a sub-sampled Yeast RNAseq dataset.</p>"},{"location":"add_a_pipeline/","title":"Add a Pipeline to the Launchpad","text":"<p>The Launchpad allows you to launch and manage Nextflow pipelines and associated compute that your pipelines will be executed on. Using the Launchpad, you can create a curated set of pipelines (including variations of the same pipeline) that are ready to be executed on the associated compute environments, while allowing the user to customize the pipeline-level parameters if needed.</p>"},{"location":"add_a_pipeline/#1-add-a-pipeline","title":"1. Add a Pipeline","text":"<p>To add a pipeline, click on the 'Add Pipeline' button. As an example, we will add the nf-core/rnaseq pipeline to the Launchpad.</p> <p></p> <p>Specify a name, description, and click on pre-existing AWS compute environment to execute on.</p>"},{"location":"add_a_pipeline/#2-specify-a-repository-url-and-revision","title":"2. Specify a repository URL and revision","text":"<p>In the repository URL, specify the nf-core/rnaseq repository:</p> <pre><code>https://github.com/nf-core/rnaseq\n</code></pre> <p>Additionally, specify a version of the pipeline as the 'Revision number'. You can use <code>3.14.0</code>.</p>"},{"location":"add_a_pipeline/#3-parameters-and-nextflow-configuration","title":"3. Parameters and Nextflow Configuration","text":"<p>Pipeline parameters and Nextflow configuration settings can also be specified as you add the pipeline to the Launchpad.</p> <p>For example, a pipeline can be pre-populated to run with specific parameters on the Launchpad. </p>"},{"location":"add_a_pipeline/#4-pre-run-script-and-additional-options","title":"4. Pre-run script and additional options","text":"<p>You can run custom code either before or after the execution of the Nextflow script. These text fields allow you to enter shell commands.</p> <p>Pre-run scripts allow you to run optional Bash script(s) that executes before pipeline launch in the same environment where Nextflow runs. Pre-run scripts are useful for executor setup (e.g., use a specific version of Nextflow) and troubleshooting. </p>"},{"location":"automation/","title":"Automation on Seqera Platform","text":"<p>Seqera Platform provides multiple methods of programmatic interaction allowing you to automate execution of pipelines, chain pipelines together, and integrate the Platform into third-party services of your choosing.</p>"},{"location":"automation/#1-seqera-platform-api","title":"1. Seqera Platform API","text":"<p>The Seqera Platform public API provides endpoints for performing all actions available on the interface, programmatically. The API can be accessed from <code>https://api.cloud.seqera.io</code>.</p> <p>The full list of endpoints is available in Seqera's OpenAPI schema found here. The API requires an authentication token to be specified in every API request. This can be created in your user menu under Your tokens.</p> <p></p> <p>The token is only displayed once. Store your token in a secure place. Use this token to authenticate requests to the API.</p> <p>For example, to launch the hello pipeline in the seqeralabs/showcase using cURL:</p> <pre><code>curl -X POST \"https://api.cloud.seqera.io/workflow/launch?workspaceId=38659136604200\" \\\n    -H \"Accept: application/json\" \\\n    -H \"Authorization: Bearer &lt;your_access_token&gt;\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Accept-Version:1\" \\\n    -d '{\n    \"launch\": {\n        \"computeEnvId\": \"hjE97A8TvD9PklUb0hwEJ\",\n        \"runName\": \"first-time-pipeline-api-byname\",\n        \"pipeline\": \"first-time-pipeline\",\n        \"workDir\": \"s3://nf-ireland\",\n        \"revision\": \"master\"\n    }\n}'\n</code></pre>"},{"location":"automation/#2-seqera-platform-cli","title":"2. Seqera Platform CLI","text":"<p>Seqera Platform comes with a command line utility called <code>tw</code> to manage resources.</p> <p>The CLI provides an interface to launch pipelines, manage compute environments, retrieve run metadata and monitor runs on the Platform. It provides a Nextflow-like experience for bioinformaticians who prefer the CLI, allows you store Seqera resource configuration (i.e. pipelines, compute environments) as code. The CLI is built on top of the Seqera Platform API but offers is simpler to use than the API. For example, you can refer to resources by name instead of unique identifier.</p> <p></p> <p>For example, to launch the hello pipeline using the CLI:</p> <pre><code>tw launch hello --workspace seqeralabs/showcase\n</code></pre> <p>The <code>tw</code> CLI installation and usage details can be obtained from this Github repository.</p>"},{"location":"automation/#3-seqerakit","title":"3. seqerakit","text":"<p><code>seqerakit</code> is a Python wrapper for the Seqera Platform CLI which can be leveraged to automate the creation of all of the entities in Seqera Platform via YAML format configuration file.</p> <p>The key features are:</p> <ul> <li>Simple configuration: All of the command-line options available when using the Seqera Platform CLI can be defined in simple YAML format.</li> <li>Infrastructure as Code: Enable users to manage and provision their infrastructure specifications.</li> <li>Automation: End-to-end creation of entities within Seqera Platform, all the way from adding an Organization to launching pipeline(s) within that Organization.</li> </ul> <p>For example, to launch the hello pipeline using seqerakit, you can create a YAML file as follows:</p> <pre><code>launch:\n  - name: \"hello-world\"\n    url: \"https://github.com/nextflow-io/hello\"\n    workspace: \"seqeralabs/showcase\"\n</code></pre> <p>The <code>seqerakit</code> installation and usage details are available on this Github repository.</p>"},{"location":"data_explorer/","title":"Data Explorer","text":"<p>With Data Explorer, you can browse and interact with remote data repositories from organization workspaces in Seqera Platform. It supports AWS S3, Azure Blob Storage, and Google Cloud Storage repositories.</p>"},{"location":"data_explorer/#view-pipeline-outputs-in-data-explorer","title":"View pipeline outputs in Data Explorer","text":"<p>In Data Explorer, you are able to:</p> <ul> <li>View bucket details:     The cloud provider, bucket address, and credentials, by selecting the information icon next to a bucket in the Data Explorer list.</li> </ul> <p></p> <ul> <li> <p>View bucket contents     Select a bucket name from the Data Explorer list to view the contents of that bucket. </p> <p>The file type, size, and path of objects are displayed in columns to the right of the object name. For example, we can take a look at the outputs of our nf-core/rnaseq run.</p> </li> </ul> <p></p> <ul> <li>Preview files:      Select a file to open a preview window that includes a Download button. For example, we can use Data Explorer to view the results of the nf-core/rnaseq pipeline that we executed. Specifically, we can take a look at the resultant gene counts of the salmon quantification step:</li> </ul> <p></p>"},{"location":"data_explorer/#configure-a-bucket-to-browser-in-data-explorer","title":"Configure a bucket to browser in Data Explorer","text":"<p>Data Explorer also enables you to add public cloud storage buckets to view and use data from resources such as:</p> <ul> <li>The Cancer Genome Atlas (TCGA)</li> <li>1000 Genomes Project</li> <li>NCBI SRA</li> <li>Genome in a Bottle Consortium</li> <li>MSSNG Database</li> <li>Genome Aggregation Database (gnomAD) </li> </ul> <p>Select 'Add cloud bucket' from the Data Explorer tab to add individual buckets (or directory paths within buckets). </p> <p>Specify the Provider, Bucket path, Name, Credentials, and Description, then select Add. For public cloud buckets, select Public from the Credentials drop-down menu.</p> <p></p> <p>You are now able to use this data in your analysis without having to interact with Cloud consoles or CLI tools. </p>"},{"location":"data_studios/","title":"Data Studios","text":"<p>Data Studios is a unified platform where you can analyse your pipeline results after successful execution.</p> <p>Data studios allow you to host a combination of images and compute environments for interactive analysis using your preferred tools, like Jupyter notebooks, RStudio, and Visual Studio Code IDEs.</p> <p>Each data studio session is an individual interactive environment that encapsulates the live environment for dynamic data analysis.</p>"},{"location":"data_studios/#data-studio-setup","title":"Data Studio Setup","text":""},{"location":"data_studios/#create-a-data-studio","title":"Create a Data Studio","text":""},{"location":"data_studios/#hidden-heading","title":"1. Add a Data Studio","text":"<p>To create a Data Studio, click on the 'Add data studio' button and select from any one of the three currently available templates.</p> <p></p>"},{"location":"data_studios/#hidden-heading","title":"2. Select a compute environment","text":"<p>Currently, only AWS Batch is supported.</p>"},{"location":"data_studios/#hidden-heading","title":"3. Mount data using Data Explorer","text":"<p>Select data to mount into your data studios environment using the Fusion file system in Data Explorer. This data will be available at <code>/workspace/data/&lt;dataset&gt;</code>.</p> <p>For example, to take a look at the results of your nf-core/rnaseq pipeline run, you can mount the value of the <code>outdir</code> parameter specified in the earlier step when launching the pipeline.</p> <p></p>"},{"location":"data_studios/#hidden-heading","title":"4. Resources for environment","text":"<p>Enter a CPU or memory allocation for your data studios environment (optional). The default is 2 CPUs and 8192 MB of memory.</p> <p>Then, click Add!</p> <p>The data studio environment will be available in the Data Studios landing page with the status 'stopped'. Click on the three dots and Start to begin running the studio.</p> <p></p> <p></p>"},{"location":"data_studios/#connect-to-a-data-studio","title":"Connect to a Data Studio","text":"<p>To connect to a running data studio session, select the three dots next to the status message and choose Connect. A new browser tab will open, displaying the status of the data studio session. Select Connect. </p>"},{"location":"data_studios/#collaborate-in-data-studio","title":"Collaborate in Data Studio","text":"<p>Collaborators can also join a data studios session in your workspace. For example, to share the results of the nf-core/rnaseq pipeline, you can share a link by selecting the three dots next to the status message for the data studio you want to share, then select Copy data studio URL. Using this link other authenticated users with the \"Connect\" role at minimum, can access the session directly.</p> <p></p>"},{"location":"data_studios/#stop-a-data-studio","title":"Stop a Data Studio","text":"<p>To stop a running session, click on the three dots next to the status and select Stop. Any unsaved analyses or results will be lost.</p> <p></p>"},{"location":"data_studios/#analyse-rnaseq-data-using-jupyter-notebooks-in-data-studios","title":"Analyse RNAseq data using Jupyter Notebooks in Data Studios","text":"<p>Data Studio can be used to perform tertiary analysis of data generated by Nextflow pipeline executions on Seqera Platform. For example, we can take a look at our nf-core/rnaseq pipeline results in a Jupyter notebook to perform additional interactive analyses.</p>"},{"location":"data_studios/#hidden-heading","title":"1. Create a Data Link","text":"<p>To enable access to our RNAseq analysis data in a Studio, we can create a custom data link pointing to the directory in our AWS S3 bucket where the results are saved. </p> <p>This can be achieved by using the 'Add cloud bucket' button in Data Explorer and specifying the path to our output directory:</p> <p></p>"},{"location":"data_studios/#hidden-heading","title":"2. Create a Jupyter notebook session","text":"<p>When creating our Data Studio, we can mount our newly created Data Link to isolate read/write access to this directory within the studio session.</p> <p></p>"},{"location":"data_studios/#hidden-heading","title":"3. Data exploration in Jupyter","text":"<p>Once created, we can Connect to our Data Studio to open a Jupyter notebook session where we can take a look at the results of our RNAseq analysis. </p> <p>For example, in the notebook, you may first want to import Python libraries:</p> <pre><code>import pandas as pd\n</code></pre> <p>We can load in our data from the analyses. For example, as a start, lets take a look at our transcript counts across the samples when loaded into a Pandas dataframe:</p> <pre><code>data = pd.read_csv('data/seqeralabs-showcase-rnaseq-results/star_salmon/salmon.merged.gene_counts.tsv', sep='\\t', index_col=0)\nprint(data.head())\n</code></pre> <p></p> <p>Through Data Studios, you are now able to continue into the next step of your tertiary analyses, using data generated from pipelines executed on Seqera Platform but stored in the Cloud - without having to ever leave the Platform.</p>"},{"location":"data_studios/#checkpoints-in-data-studios","title":"Checkpoints in Data studios","text":"<p>When starting a data studio, a checkpoint gets created. This checkpoint allows you to restart a data studio with previously installed software and changes made to the root filesystem of the container. Please note, that if you stop a data studio and restart it, this will restart it from the latest checkpoint. To go back to a specific previous configuration of data studio session, please restart it from a checkpoint as highlighted in the screenshot below:</p> <p></p>"},{"location":"data_studios/#more-information","title":"More information","text":"<p>If you want more detailed explanation about specific concepts of Data Studios or find out which tools are preinstalled in Data Studio images, please visit Seqera Platform Docs</p>"},{"location":"demo_overview/","title":"Getting started","text":""},{"location":"demo_overview/#1-login-to-seqeraio","title":"1. Login to seqera.io","text":"<p>Log into Seqera Platform, either through a GitHub account, Google account, or an email address.</p> <p>Upon providing an email address, Seqera Cloud will send an authentication link, enabling login.</p> <p></p>"},{"location":"demo_overview/#2-navigate-into-the-seqeralabsshowcase-workspace","title":"2. Navigate into the seqeralabs/showcase Workspace","text":"<p>All resources in Seqera Platform live inside a Workspace, which in turn belong to an Organization. Typically, teams of colleagues or collaborators will share one or more workspaces. All resources in a Workspace (i.e. pipelines, compute environments, datasets) are shared by members of that workspace.</p> <p>Navigate into the <code>seqeralabs/showcase</code> Workspace.</p> <p></p>"},{"location":"demo_overview/#3-user-settings","title":"3. User settings","text":"<p>To access or modify your user settings such as your username, or name, click on the avatar icon in the top right corner. You will be able to modify these settings in 'Your profile'.</p> <p></p> <p>You can specify user specific settings such as:</p> <ul> <li>User tokens: Your personal access token for authentication on the Platform, used in automation.</li> <li>User credentials: Credentials for your own personal workspace which can include cloud access keys, repository credentials, Docker credentials.</li> <li>User secrets: Secrets used in any Nextflow workflows launched in your user workspace.</li> </ul>"},{"location":"intro/","title":"Why use Seqera Platform?","text":""},{"location":"intro/#introduction-to-nextflow-and-seqera-platform","title":"Introduction to Nextflow and Seqera Platform","text":""},{"location":"intro/#what-is-nextflow","title":"What is Nextflow?","text":"<p>Nextflow is a workflow system for creating scalable, portable, and reproducible workflows.</p> <p>Nextflow is both a workflow language and an execution runtime that supports a wide range of execution platforms, including popular traditional grid scheduling systems such as Slurm and IBM LSF, and cloud services such as AWS Batch, Google Cloud Batch, Azure Batch and Kubernetes.</p>"},{"location":"intro/#running-a-nextflow-pipeline","title":"Running a Nextflow pipeline","text":"<p>Nextflow provides a simple command line interface for managing and executing pipelines.</p> <p>Let's take a look at how to run a basic, Nextflow pipeline using a simple Hello World script.</p> <ol> <li>First, ensure Nextflow is installed:</li> </ol> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <ol> <li>With Nextflow installed, you can then run the following on your command-line to start running the hello pipeline:</li> </ol> <pre><code>nextflow run https://github.com/nextflow-io/hello\n</code></pre>"},{"location":"intro/#monitoring-and-finding-logs","title":"Monitoring and Finding logs","text":"<p>When you run a Nextflow pipeline via the CLI with <code>nextflow run</code>, it generates logs that can be used to monitor the execution of the pipeline. The logs are printed to the console, and detailed execution trace can be found in the work directory created by Nextflow. Each execution generates its own directory under work, where logs and output files are stored.</p> <ul> <li>Execution Log: The main log file (<code>nextflow.log</code>) is created in the directory where Nextflow is run. This file captures detailed information about the pipeline execution, including system errors and warnings.</li> <li>Command Log: Within the work directory, each process execution generates a <code>.command.log</code> file, which contains the standard output and error streams of the executed command.</li> </ul>"},{"location":"intro/#limitations-of-cli","title":"Limitations of CLI","text":"<p>Monitoring and launching via CLI, though direct, poses challenges, especially with complex or large-scale pipelines that are not as simple as just running Hello World:</p> <ul> <li>Scalability: As the number of tasks increases, manually checking individual log files becomes impractical.</li> <li>Real-Time Tracking: The CLI does not offer an easy way to visualize real-time progress across multiple parallel tasks.</li> <li>Aggregation: Collecting and interpreting logs from various processes requires additional tools or scripts, complicating the workflow management.</li> <li>Flexibility: Switching between environments (i.e. your local computer to HPC, or cloud) requires the setup of access in the form of account keys and credentials to the environment on your CLI, followed by using the appropriate Nextflow configuration settings.</li> </ul>"},{"location":"intro/#enhancing-management-of-pipelines-in-seqera-platform","title":"Enhancing management of Pipelines in Seqera Platform","text":"<p>Seqera Platform extends the capabilities of Nextflow by providing advanced monitoring, and pipeline and data management tools:</p> <ul> <li>Centralized Monitoring Dashboard: A user-friendly interface displays all critical information, including real-time progress of each pipeline.</li> <li>Easily Accessible Run Details: Seqera Platform captures every detail about a pipeline run, including the exact parameters and configurations used, ensuring full reproducibility.</li> <li>Resource Usage Metrics: It provides comprehensive metrics on resource usage for each task, crucial for optimizing cloud executions and managing costs effectively. These metrics are presented in an accessible format, contrasting with the complexity of extracting and interpreting them from CLI logs.</li> <li>Explore and Manage Data: The Platform makes it easier to manage data across disparate sources for your pipeline executions without having to use Cloud consoles or CLI utilities.</li> <li>Analyze your Data: Interactive notebooks, RStudios environments, and VSCode streamline the analysis of your data generated from pipeline executions.</li> </ul> <p>This guide will demonstrate the various features of Seqera Platform which makes it easier to build, launch, and manage scalable data pipelines.</p>"},{"location":"launch_pipeline/","title":"Launch the nf-core/rnaseq pipeline","text":""},{"location":"launch_pipeline/#1-go-to-launchpad","title":"1. Go to Launchpad","text":"<p>Navigate back to the Launchpad to begin executing the newly added nf-core/rnaseq pipeline.</p> <p>Select 'Launch' next to the pipeline of your choice to open the pipeline launch form.</p> <p></p> <p>Seqera uses a nextflow_schema.json file in the root of the pipeline repository to dynamically create a form with the necessary pipeline parameters.</p>"},{"location":"launch_pipeline/#2-overview-of-the-launch-form","title":"2. Overview of the Launch form","text":"<p>All pipelines contain at least these parameters:</p> <p>1. Workflow run name: A unique identifier for the run, pre-filled with a random name. This can be customized.</p> <p>2. Labels: Assign new or existing labels to the run.</p> <p>3. Input/output options: Specify paths to pipeline input datasets, output directories, and other pipeline-specific I/O options. Input and outdir are required fields common to all pipelines:</p> <p>For the 'input' parameter, click on the text box and click on the name of the dataset added in the previous step.</p> <p></p> <p>For the 'outdir' parameter, specify an S3 directory path manually, or select Browse to specify a cloud storage directory using Data Explorer.</p> <p></p> <p>The remaining fields of the pipeline parameters form will vary for each pipeline, dependent on the parameters specified in the pipeline schema. When you have filled the necessary launch form details, select 'Launch'.</p>"},{"location":"monitor_run/","title":"View the Runs","text":"<p>Upon launching, you'll be navigated to the 'Runs' tab which contains all executed workflows. Click on the workflow executed in the previous step.</p> <p>The Runs tab contains all previous job executions. Each new or resumed job is given a random name, e.g., \"grave_williams\". Each row corresponds to a specific job. As a job executes, it can transition through the following states:</p> <ul> <li>submitted: Pending execution</li> <li>running: Running</li> <li>succeeded: Completed successfully</li> <li>failed: Successfully executed, where at least one task failed with a terminate error strategy</li> <li>cancelled: Stopped forceably during execution   unknown: Indeterminate status</li> </ul> <p></p> <p>As the pipeline begins to run, you will see the Runs page become populated with the following details:</p> <ul> <li>Command-line invocation for the Run</li> <li>Parameters specified to the pipeline</li> <li>Resolved Nextflow configuration</li> <li>Execution Log</li> <li>Datasets used, and Reports generated</li> </ul> <p></p>"},{"location":"monitor_run/#1-view-run-info","title":"1. View Run info","text":"<p>On the Runs page will be General information about who executed the run, when, the Git hash used and tag, as well as additional details about the compute environment used, and the version of Nextflow.</p> <p></p>"},{"location":"monitor_run/#2-view-reports","title":"2. View Reports","text":"<p>Most Nextflow pipelines will generate reports or output files which are useful to inspect at the end of the pipeline execution. Reports can contain quality control (QC) metrics that would be important to assess the integrity of the results.</p> <p>Reports allow you to directly visualise supported file types or to download them via the user interface. This saves users the time and effort of having to retrieve and visualize output files from their local storage. Once the pipeline completes, you can view the outputs of the pipeline in the 'Reports' tab.</p> <p>For example, for the nf-core/rnaseq pipeline, you can view the MultiQC report generated.</p> <p></p> <p></p>"},{"location":"monitoring_views/","title":"Monitoring views","text":"<p>Workflow executions submitted in Seqera Platform can be monitored wherever you have an internet connection.</p>"},{"location":"monitoring_views/#1-all-runs-view","title":"1. All runs view","text":"<p>The All runs page, accessed from the user menu, provides a comprehensive overview of the runs accessible to a user across the entire Seqera instance. The default view will be all organizations and workspaces you have access to. You can filter these by selecting the dropdown next to View or searching for a run. You can filter based on the following entries:</p> <ul> <li><code>status</code></li> <li><code>label</code></li> <li><code>workflowId</code></li> <li><code>runName</code></li> <li><code>username</code></li> <li><code>projectName</code></li> <li><code>after: YYYY-MM-DD</code></li> <li><code>before: YYYY-MM-DD</code></li> <li><code>sessionId</code></li> <li><code>is:starred</code></li> </ul> <p>For example:</p> <pre><code>rnaseq username:johndoe status:succeeded after:2024-01-01\n</code></pre> <p></p>"},{"location":"monitoring_views/#2-dashboard-view","title":"2. Dashboard view","text":"<p>The Seqera Platform Dashboard, accessed from the user menu, provides an overview of runs in your personal and organization workspaces. The default view will be all organizations and workspaces you have acces to. You can filter for these by selecting the dropdown next to View and filter by time, including a custom date range up to 12 months.</p> <p>You can also export this data in CSV format by clicking the Export data button.</p> <p></p>"},{"location":"pipeline_optimization/","title":"Optimize the Pipeline","text":"<p>Seqera's pipeline optimization feature uses resource usage information from previous runs to minimize the resources used in your pipeline runs.</p> <p>Optimization is available for pipelines once 1 successful run has been completed. This will be indicated by the grey-lightbulb icon turning into a black-hashed lightbulb that will allow you to view the optimized profile. </p> <p>This profile consists of Nextflow configuration settings for each process and each resource directive (where applicable): cpus, memory, and time. The optimized setting for a given process and resource directive is based on the maximum use of that resource across all tasks in that process.</p> <p>Once optimization is selected, any subsequent runs of that pipeline on the Launchpad will inherit the optimized configuration profile, indicated by the black lightbulb icon with a checkmark. </p> <p>NOTE: Optimizated profiles are generated off of one run at a time, defaulting to the most recent runs, and not an aggregation of previous runs.</p> <p>Navigate back to the Launchpad, click on the nf-core/rnaseq Pipeline added, and click on the 'Lightbulb' icon to view the optimized profile. You have the flexibility to tailor the optimization's target settings and incorporate a retry strategy as needed.</p> <p></p> <p>You can verify the optimized configuration of a given run by inspecting the resource usage plots for that run and these fields in the run's task table:</p> Description Key CPU usage <code>pcpu</code> Memory usage <code>peakRss</code> Runtime <code>start</code> and <code>complete</code>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#hidden-heading","title":"Seqera","text":"<p> About us</p>"},{"location":"resources/#hidden-heading","title":"Seqera Platform","text":"<p> Seqera Platform</p> <p> Nextflow documentation</p>"},{"location":"resources/#hidden-heading","title":"Blog Posts","text":"<p> Best Practices for Deploying Pipelines with the Seqera Platform (formerly Nextflow Tower)</p> <p> Breakthrough performance and cost-efficiency with the new Fusion file system</p> <p> Workflow Automation for Nextflow Pipelines</p>"},{"location":"resume_pipeline/","title":"Resuming a Run","text":"<p>Seqera Platform enables you to use Nextflow's resume functionality to resume a workflow run with the same parameters, using the cached results of previously completed tasks and only executing failed and pending tasks.</p>"},{"location":"resume_pipeline/#1-resume","title":"1. Resume","text":"<p>To resume a failed or cancelled run:</p> <ul> <li>Click on the three dots next to the Run</li> <li>Select 'Resume' from the options menu</li> <li>Edit the parameters before launch, if desired</li> <li>If you have the appropriate permissions, you may edit the compute environment if desired</li> </ul> <p>Notes:</p> <ul> <li>Unlike a relaunch, you cannot edit the Pipeline to launch or the work directory during a run resume.</li> <li>When changing the compute environment, the new compute environment must have a work directory that matches the root path of the original pipeline work directory. For example, if the original pipeline work directory is <code>s3://foo/work/12345</code>, the new compute environment must have access to <code>s3://foo/work</code>.</li> </ul>"},{"location":"resume_pipeline/#2-monitor-progress","title":"2. Monitor progress","text":"<p>Check the Runs page for the newly submitted run and monitor progress. In the Run details page, the status of tasks will be updated in real time as they progress from 'Submitted' to 'Running' to 'Succeeded' or 'Failed'. If you are resuming a run that had tasks that were completed successfully, you will see a number of tasks shown as 'Cached'.</p> <p></p>"},{"location":"run_details/","title":"Run and Task Details","text":""},{"location":"run_details/#1-general-summary","title":"1. General summary","text":"<p>On the Runs page, the General panel displays top-level information about a pipeline run:</p> <ul> <li>Unique workflow run ID</li> <li>Workflow run name</li> <li>Timestamp of pipeline start</li> <li>Project revision and Git commit ID</li> <li>Nextflow session ID</li> <li>Username of the launcher</li> <li>Work directory path</li> </ul>"},{"location":"run_details/#2-view-details-for-a-task","title":"2. View details for a Task","text":"<p>Scroll down the Runs page and you will see:</p> <ul> <li>The progress of each Process in the pipeline</li> <li>Aggregated stats for the Run (i.e. total walltime, CPU hours)</li> <li>Workflow metrics (i.e. CPU efficiency, memory efficiency)</li> <li>Task details table for every task in the workflow</li> </ul>"},{"location":"run_details/#3-task-details-window","title":"3. Task details window","text":"<p>Select a task in the task table to open the Task details dialog. The dialog has three tabs: About, Execution log and Data Explorer.</p> <p>About</p> <p>The About tab provides the following information:</p> <ol> <li> <p>Name: Process name and tag</p> </li> <li> <p>Command: Task script, defined in the pipeline process</p> </li> <li> <p>Status: Exit code, task status, attempts</p> </li> <li> <p>Work directory: Directory where the task was executed</p> </li> <li> <p>Environment: Environment variables that were supplied to the task</p> </li> <li> <p>Execution time: Metrics for task submission, start, and completion time</p> </li> <li> <p>Resources requested: Metrics for the resources requested by the task</p> </li> <li> <p>Resources used: Metrics for the resources used by the task</p> </li> </ol> <p></p> <p>Execution log</p> <p>The Execution log tab provides a real-time log of the selected task's execution. Task execution and other logs (such as stdout and stderr) are available for download from here, if still available in your compute environment.</p>"},{"location":"run_details/#4-task-details-in-data-explorer","title":"4. Task details in Data Explorer","text":"<p>The Data Explorer tab allows you to view the log files and output files generated for each task in it's working directory within the Platform.</p> <p>You can view, download, and retrieve the link for these intermediate files stored in the Cloud from the Explorer tab.</p> <p></p>"},{"location":"summary/","title":"Seqera Platform","text":""},{"location":"summary/#hidden-heading","title":"One platform for the scientific data analysis life cycle","text":"<p>Throughout this guide, you have experienced how the Seqera Platform streamlines the management, execution, monitoring, and analysis of Nextflow pipelines in the cloud. This centralized and intuitive interface offers numerous advantages:</p> <p> Ease of Access: Enables all users to execute Nextflow pipelines with ease.</p> <p> Simplified Cloud Deployment: Allows for the deployment of pipelines on the cloud without needing to understand the underlying infrastructure.</p> <p> Real-Time Monitoring: Provides the ability to view the progress and outcomes of pipeline runs directly, bypassing the need for direct access to the execution environment.</p> <p> Enhanced Provenance Tracking: Facilitates the logging and tracking of pipeline provenance, enhancing reproducibility.</p> <p> Cloud Data Interaction: Supports seamless interaction with cloud-stored data, eliminating the need for direct cloud console or CLI interactions.</p> <p> Automated Resource Management: Reduces manual resource tuning, preventing allocation errors and optimizing task execution.</p> <p> Collaborative Efficiency: Boosts productivity by enabling researchers to share, collaborate, and interpret results effortlessly, without additional infrastructure overhead.</p> <p>Seqera Platform empowers scientists to conduct high-throughput computing on a large scale, utilizing modern software engineering practices, all from a single, unified location. This guide has outlined how leveraging these capabilities can transform your research productivity and computational efficiency.</p>"}]}